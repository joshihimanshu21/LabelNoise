{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[313]:\n",
    "\n",
    "\n",
    "#import all modules here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model as sk\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import randint\n",
    "from IPython.display import display\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Approach3(data_df,label_column, N, noise_frac):\n",
    "\n",
    "    def add_noise(data, col):\n",
    "        class0 = data.loc[data[col]!=1,:]\n",
    "        class1 = data.loc[data[col]!=0,:]\n",
    "        def noise(data):\n",
    "            sample = data.sample(frac = noise_frac)\n",
    "            for i in list(sample.index):\n",
    "                data[col][i] = int(not data[col][i])\n",
    "            return list(sample.index)    \n",
    "        noise0 = noise(class0)\n",
    "        noise1 = noise(class1)\n",
    "        noisy_indices = noise0+noise1\n",
    "        mislabeled = pd.concat([class0,class1])\n",
    "        indices = list(mislabeled.index)\n",
    "        return mislabeled,noisy_indices\n",
    "\n",
    "\n",
    "    # In[317]:\n",
    "\n",
    "\n",
    "    mislabeled,noisy_indices = add_noise(data_df, label_column)\n",
    "\n",
    "\n",
    "    # In[318]:\n",
    "\n",
    "\n",
    "    def distance_from_centroid(df,centroids,data_label):\n",
    "        d = defaultdict(list)\n",
    "        for index in df.index:\n",
    "            dist = 0\n",
    "            for i in range(len(centroids[0])):\n",
    "                dist+=(centroids[df.loc[index,'cluster']][i] - df.loc[index,df.columns[i]])**2\n",
    "            d[df.loc[index,'cluster']].append([dist,index,data_label[index]])\n",
    "        return d\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In[319]:\n",
    "\n",
    "\n",
    "    def ratio_in_cluster(features,data_label,n):\n",
    "        majority_label_in_cluster = defaultdict(None)\n",
    "        km = KMeans(n_clusters = n)\n",
    "        km.fit(features)\n",
    "        label = km.labels_\n",
    "        print(len(label))\n",
    "        print(features.shape)\n",
    "        features['cluster'] = label\n",
    "\n",
    "        distance = distance_from_centroid(features,km.cluster_centers_,data_label)\n",
    "\n",
    "        for i in distance.keys():\n",
    "            distance[i] = sorted(distance[i],key=lambda x: x[0])\n",
    "\n",
    "        normalized_distance = dict(distance)\n",
    "        label_ratio_count = {}\n",
    "\n",
    "        for i in normalized_distance:\n",
    "            temp = [0,0]\n",
    "            for j in range(len(normalized_distance[i])):\n",
    "                normalized_distance[i][j][0] = normalized_distance[i][j][0]/normalized_distance[i][-1][0]\n",
    "                temp[normalized_distance[i][j][-1]] +=1\n",
    "\n",
    "            temp[0],temp[1] = temp[0]/sum(temp),temp[1]/sum(temp)\n",
    "            label_ratio_count[i] = temp\n",
    "\n",
    "            if temp[0] > temp[1] and temp[0]>=0.7:\n",
    "                majority_label_in_cluster[i] = 0\n",
    "            elif temp[0] < temp[1] and temp[1]>=0.7:\n",
    "                majority_label_in_cluster[i] = 1\n",
    "            else:\n",
    "                majority_label_in_cluster[i] = None\n",
    "\n",
    "        return(normalized_distance, majority_label_in_cluster)\n",
    "        #return pd.DataFrame(data=label_ratio_count),majority_label_in_cluster  \n",
    "\n",
    "\n",
    "\n",
    "    # In[320]:\n",
    "\n",
    "\n",
    "    normalized_distance, majority_label_in_cluster = ratio_in_cluster(mislabeled.loc[:,mislabeled.columns!=label_column],mislabeled[label_column],N)\n",
    "\n",
    "\n",
    "    # In[321]:\n",
    "\n",
    "\n",
    "    points_removed = []\n",
    "    reduced_points = defaultdict(list)\n",
    "    for cluster_id in normalized_distance.keys():\n",
    "        count = 0\n",
    "        for point in normalized_distance[cluster_id]:\n",
    "            if count/len(normalized_distance[cluster_id]) > 0.75:\n",
    "                if point[-1] != majority_label_in_cluster[cluster_id] and majority_label_in_cluster[cluster_id]!=None:\n",
    "                    points_removed.append(point)\n",
    "                    continue\n",
    "                else:\n",
    "                    reduced_points[cluster_id].append(point)\n",
    "                    count+=1\n",
    "            else:\n",
    "                reduced_points[cluster_id].append(point)\n",
    "                count +=1\n",
    "\n",
    "\n",
    "    # In[322]:\n",
    "\n",
    "\n",
    "    # result = []\n",
    "    # for cluster_id in normalized_distance:\n",
    "    #     print(len(normalized_distance[cluster_id]), len(reduced_points[cluster_id]))\n",
    "    #     print((len(reduced_points[cluster_id]) * 1.0) -  len(normalized_distance[cluster_id]))\n",
    "    #     print()\n",
    "\n",
    "\n",
    "    # In[323]:\n",
    "\n",
    "\n",
    "    #reduced_points\n",
    "\n",
    "\n",
    "    # In[324]:\n",
    "\n",
    "\n",
    "    #majority_label_in_cluster\n",
    "\n",
    "\n",
    "    # In[325]:\n",
    "\n",
    "\n",
    "    #points_removed\n",
    "\n",
    "\n",
    "    # In[326]:\n",
    "\n",
    "\n",
    "    noisy_indices = set(noisy_indices)\n",
    "    n_n = 0\n",
    "    p_n = 0\n",
    "    for point in points_removed:\n",
    "        if point[2] in noisy_indices:\n",
    "            n_n+=1\n",
    "        else:\n",
    "            p_n+=1\n",
    "    p_p = (len(mislabeled) - len(noisy_indices)) - p_n\n",
    "    n_p = len(noisy_indices) - n_n\n",
    "    print(\"# Observation in Culprit\",len(points_removed))\n",
    "    print(\"Noise to Noise\",n_n)\n",
    "    print(\"Pure to Noise\",p_n)\n",
    "    print(\"Noise to Pure\",n_p)\n",
    "    print(\"Pure to Pure\",p_p)\n",
    "    print('Pure %',100 * p_p/(len(mislabeled)-len(points_removed)))\n",
    "    print('Noisy %',100 * n_p/(len(mislabeled)-len(points_removed)))\n",
    "    print('Cleaned %',100 * n_n/(len(noisy_indices)))\n",
    "\n",
    "\n",
    "    # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 14\n",
    "# data_df = pd.read_csv(\"../haberman.data\",header=None)\n",
    "# data_df.fillna(data_df.median(),inplace=True)\n",
    "# data_df.columns = ['Age','Year','Nodes','Survival_status']\n",
    "# data_df['Survival_status'] = data_df['Survival_status'].astype(int)\n",
    "# data_df['Survival_status'] = data_df['Survival_status'].map({2:0 , 1:1})\n",
    "# label_column = 'Survival_status'\n",
    "# noise_frac = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 14\n",
    "data_df = pd.read_csv('../SPECT.train',header=None)\n",
    "label_column = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "80\n",
      "(80, 22)\n",
      "# Observation in Culprit 3\n",
      "Noise to Noise 0\n",
      "Pure to Noise 3\n",
      "Noise to Pure 8\n",
      "Pure to Pure 69\n",
      "Pure % 89.6103896103896\n",
      "Noisy % 10.38961038961039\n",
      "Cleaned % 0.0\n",
      "\n",
      "0.2\n",
      "80\n",
      "(80, 22)\n",
      "# Observation in Culprit 2\n",
      "Noise to Noise 0\n",
      "Pure to Noise 2\n",
      "Noise to Pure 16\n",
      "Pure to Pure 62\n",
      "Pure % 79.48717948717949\n",
      "Noisy % 20.512820512820515\n",
      "Cleaned % 0.0\n",
      "\n",
      "0.3\n",
      "80\n",
      "(80, 22)\n",
      "# Observation in Culprit 0\n",
      "Noise to Noise 0\n",
      "Pure to Noise 0\n",
      "Noise to Pure 24\n",
      "Pure to Pure 56\n",
      "Pure % 70.0\n",
      "Noisy % 30.0\n",
      "Cleaned % 0.0\n",
      "\n",
      "0.4\n",
      "80\n",
      "(80, 22)\n",
      "# Observation in Culprit 0\n",
      "Noise to Noise 0\n",
      "Pure to Noise 0\n",
      "Noise to Pure 32\n",
      "Pure to Pure 48\n",
      "Pure % 60.0\n",
      "Noisy % 40.0\n",
      "Cleaned % 0.0\n",
      "\n",
      "0.5\n",
      "80\n",
      "(80, 22)\n",
      "# Observation in Culprit 0\n",
      "Noise to Noise 0\n",
      "Pure to Noise 0\n",
      "Noise to Pure 40\n",
      "Pure to Pure 40\n",
      "Pure % 50.0\n",
      "Noisy % 50.0\n",
      "Cleaned % 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print(i/10)\n",
    "    Approach3(data_df,label_column,N,i/10)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
