{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all modules here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model as sk\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import randint\n",
    "from IPython.display import display\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 14\n",
    "# data_df = pd.read_csv('../SPECT.train',header=None)\n",
    "# features = df.loc[:,df.columns!=0]\n",
    "# label = df[0]\n",
    "# label_column = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 14\n",
    "data_df = pd.read_csv(\"../haberman.data\",header=None)\n",
    "data_df.fillna(data_df.median(),inplace=True)\n",
    "data_df.columns = ['Age','Year','Nodes','Survival_status']\n",
    "data_df['Survival_status'] = data_df['Survival_status'].astype(int)\n",
    "data_df['Survival_status'] = data_df['Survival_status'].map({2:0 , 1:1})\n",
    "label_column = 'Survival_status'\n",
    "noise_frac = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data, col):\n",
    "    class0 = data.loc[data[col]!=1,:]\n",
    "    class1 = data.loc[data[col]!=0,:]\n",
    "    def noise(data):\n",
    "        sample = data.sample(frac = noise_frac)\n",
    "        for i in list(sample.index):\n",
    "            data[col][i] = int(not data[col][i])\n",
    "        return list(sample.index)    \n",
    "    noise0 = noise(class0)\n",
    "    noise1 = noise(class1)\n",
    "    noisy_indices = noise0+noise1\n",
    "    mislabeled = pd.concat([class0,class1])\n",
    "    indices = list(mislabeled.index)\n",
    "    return mislabeled,noisy_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled,noisy_indices = add_noise(data_df, label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_from_centroid(df,centroids,data_label):\n",
    "    d = defaultdict(list)\n",
    "    for index in df.index:\n",
    "        dist = 0\n",
    "        for i in range(len(centroids[0])):\n",
    "            dist+=(centroids[df.loc[index,'cluster']][i] - df.loc[index,df.columns[i]])**2\n",
    "        d[df.loc[index,'cluster']].append([dist,index,data_label[index]])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_in_cluster(features,data_label,n):\n",
    "    majority_label_in_cluster = defaultdict(None)\n",
    "    km = KMeans(n_clusters = n)\n",
    "    km.fit(features)\n",
    "    label = km.labels_\n",
    "    print(len(label))\n",
    "    print(features.shape)\n",
    "    features['cluster'] = label\n",
    "    \n",
    "    distance = distance_from_centroid(features,km.cluster_centers_,data_label)\n",
    "    \n",
    "    for i in distance.keys():\n",
    "        distance[i] = sorted(distance[i],key=lambda x: x[0])\n",
    "        \n",
    "    normalized_distance = dict(distance)\n",
    "    label_ratio_count = {}\n",
    "    \n",
    "    for i in normalized_distance:\n",
    "        temp = [0,0]\n",
    "        for j in range(len(normalized_distance[i])):\n",
    "            normalized_distance[i][j][0] = normalized_distance[i][j][0]/normalized_distance[i][-1][0]\n",
    "            temp[normalized_distance[i][j][-1]] +=1\n",
    "\n",
    "        temp[0],temp[1] = temp[0]/sum(temp),temp[1]/sum(temp)\n",
    "        label_ratio_count[i] = temp\n",
    "        \n",
    "        if temp[0] > temp[1] and temp[0]>=0.7:\n",
    "            majority_label_in_cluster[i] = 0\n",
    "        elif temp[0] < temp[1] and temp[1]>=0.7:\n",
    "            majority_label_in_cluster[i] = 1\n",
    "        else:\n",
    "            majority_label_in_cluster[i] = None\n",
    "\n",
    "    return(normalized_distance, majority_label_in_cluster)\n",
    "    #return pd.DataFrame(data=label_ratio_count),majority_label_in_cluster  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n",
      "(306, 3)\n"
     ]
    }
   ],
   "source": [
    "normalized_distance, majority_label_in_cluster = ratio_in_cluster(mislabeled.loc[:,mislabeled.columns!=label_column],mislabeled[label_column],N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "points_removed = []\n",
    "reduced_points = defaultdict(list)\n",
    "for cluster_id in normalized_distance.keys():\n",
    "    count = 0\n",
    "    for point in normalized_distance[cluster_id]:\n",
    "        if count/len(normalized_distance[cluster_id]) > 0.75:\n",
    "            if point[-1] != majority_label_in_cluster[cluster_id] and majority_label_in_cluster[cluster_id]!=None:\n",
    "                points_removed.append(point)\n",
    "                continue\n",
    "            else:\n",
    "                reduced_points[cluster_id].append(point)\n",
    "                count+=1\n",
    "        else:\n",
    "            reduced_points[cluster_id].append(point)\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = []\n",
    "# for cluster_id in normalized_distance:\n",
    "#     print(len(normalized_distance[cluster_id]), len(reduced_points[cluster_id]))\n",
    "#     print((len(reduced_points[cluster_id]) * 1.0) -  len(normalized_distance[cluster_id]))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority_label_in_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Observation in Culprit 5\n",
      "Noise to Noise 5\n",
      "Pure to Noise 0\n",
      "Noise to Pure 87\n",
      "Pure to Pure 214\n",
      "Pure % 71.09634551495017\n",
      "Noisy % 28.903654485049834\n",
      "Cleaned % 5.434782608695652\n"
     ]
    }
   ],
   "source": [
    "noisy_indices = set(noisy_indices)\n",
    "n_n = 0\n",
    "p_n = 0\n",
    "for point in points_removed:\n",
    "    if point[2] in noisy_indices:\n",
    "        n_n+=1\n",
    "    else:\n",
    "        p_n+=1\n",
    "p_p = (len(mislabeled) - len(noisy_indices)) - p_n\n",
    "n_p = len(noisy_indices) - n_n\n",
    "print(\"# Observation in Culprit\",len(points_removed))\n",
    "print(\"Noise to Noise\",n_n)\n",
    "print(\"Pure to Noise\",p_n)\n",
    "print(\"Noise to Pure\",n_p)\n",
    "print(\"Pure to Pure\",p_p)\n",
    "print('Pure %',100 * p_p/(len(mislabeled)-len(points_removed)))\n",
    "print('Noisy %',100 * n_p/(len(mislabeled)-len(points_removed)))\n",
    "print('Cleaned %',100 * n_n/(len(noisy_indices)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
